{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL - LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "TOGETHER_API_KEY = \"41bad9b287c02b34b4a844bcfc21f18225cc0852f983652c8952908795a75c07\"\n",
    "\n",
    "openai.api_key = TOGETHER_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Cleans the response to a simple string\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_base=\"https://api.together.xyz/v1\",\n",
    "    openai_api_key=TOGETHER_API_KEY,\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chain: LCEL\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Why don't bears like to play hide and seek?\\nBecause they always get spotted!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Complex Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Vector Stores\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "import numpy as np\n",
    "\n",
    "# we make a class for the TogetherAI embedding (instead of OpenAI's embedding)\n",
    "class TogetherAIEmbeddings(Embeddings):\n",
    "    \n",
    "    \"\"\"Custom langchain embedding class for TogetherAI\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model=\"togethercomputer/m2-bert-80M-8k-retrieval\", # Embedding model\n",
    "        api_key=None\n",
    "    ):\n",
    "        self.client = Together(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    # Embedding a full document    \n",
    "    def embed_documents(self, texts):\n",
    "        # embed multiple documents\n",
    "        response = self.client.embeddings.create(model=self.model, input=texts)\n",
    "        return [x.embedding for x in response.data]\n",
    "\n",
    "    # Embedding a single query\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embeds a single query\"\"\"\n",
    "        response = self.client.embeddings.create(model=self.model, input=[text])  # pass as list of inputs\n",
    "        return response.data[0].embedding # Extract the first embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.embeddings.create(\n",
    "  model = \"togethercomputer/m2-bert-80M-8k-retrieval\",\n",
    "  input = [\n",
    "    \"Our solar system orbits the Milky Way galaxy at about 515,000 mph\",\n",
    "    \"Jupiter's Great Red Spot is a storm that has been raging for at least 350 years.\"\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x.embedding for x in response.data][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = TogetherAIEmbeddings(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorstore with OSS Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerlshin/anaconda3/envs/gnn/lib/python3.9/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    texts=[\"harrison worked at kensho\", \"bears like to eat honey\"], # Content\n",
    "    embedding=TogetherAIEmbeddings(api_key=TOGETHER_API_KEY)  # Embedding model\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where did Harrison work?\"\n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='harrison worked at kensho'),\n",
       " Document(metadata={}, page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20631/2656275837.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\"what do bears like to eat\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='bears like to eat honey'),\n",
       " Document(metadata={}, page_content='harrison worked at kensho')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what do bears like to eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessor. It runs multiple functions in parallel to prepare the data for the next step in the chain\n",
    "from langchain.schema.runnable import RunnableMap \n",
    "\n",
    "# Mapping inputs to Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableMap(\n",
    "    # lambda function mapping\n",
    "    {\n",
    "        \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]), # fetches the relevant text from the vector db\n",
    "        \"question\": lambda x: x[\"question\"] # original string\n",
    "    }\n",
    ") | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, Harrison works at Kensho.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Where di harrison work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='bears like to eat honey')],\n",
       " 'question': 'where did harrison work?'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Binding\n",
    "\n",
    "* Process of telling the LLM the list of tools it is allowed to use for user's request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"weather_search\",\n",
    "        \"description\": \"Search for weather given an airport code\",\n",
    "        \n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \n",
    "            \"properties\": {\n",
    "                \"airport_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The airport code to get the weather for\"\n",
    "                },\n",
    "            },\n",
    "            \n",
    "            \"required\": [\"airport_code\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    openai_api_base=\"https://api.together.xyz/v1\",\n",
    "    openai_api_key=TOGETHER_API_KEY,\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    ").bind(functions=functions) # Attaching these function definition to every request sent to the AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    # User input and prepares it with RunnableMap\n",
    "    RunnableMap({\"input\": lambda x: x[\"input\"]})  # Pass question as input\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"input\": \"what is the weather in sf\"}\n",
    "\n",
    "response = chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_er1iv94b7wg9cbw34t8mxqpl', 'function': {'arguments': '{\"airport_code\":\"SF\"}', 'name': 'weather_search'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 488, 'total_tokens': 523, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'mistralai/Mistral-7B-Instruct-v0.1', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-2a09afd5-d294-4cd5-bee0-ae99a37305ea-0' tool_calls=[{'name': 'weather_search', 'args': {'airport_code': 'SF'}, 'id': 'call_er1iv94b7wg9cbw34t8mxqpl', 'type': 'tool_call'}] usage_metadata={'input_tokens': 488, 'output_tokens': 35, 'total_tokens': 523, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"Extract the airport code from: 'What is the weather in SF?'\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" I'm sorry, but I don't have information on recent sport events. My current capabilities are limited to searching for weather based on an airport code.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 848, 'total_tokens': 881, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'mistralai/Mistral-7B-Instruct-v0.1', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None}, id='run-0eb50a49-adfc-456c-b462-e4e2d7aaf37f-0', usage_metadata={'input_tokens': 848, 'output_tokens': 33, 'total_tokens': 881, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
