{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module):\n",
    "  \"\"\"\n",
    "  A dummy model that consists of an embedding layer\n",
    "  with two blocks of a linear layer followed by a layer\n",
    "  norm layer.\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    self.token_embedding = nn.Embedding(2, 2)\n",
    "\n",
    "    # Block 1\n",
    "    self.linear_1 = nn.Linear(2, 2)\n",
    "    self.layernorm_1 = nn.LayerNorm(2)\n",
    "\n",
    "    # Block 2\n",
    "    self.linear_2 = nn.Linear(2, 2)\n",
    "    self.layernorm_2 = nn.LayerNorm(2)\n",
    "\n",
    "    self.head = nn.Linear(2, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    hidden_states = self.token_embedding(x)\n",
    "\n",
    "    # Block 1\n",
    "    hidden_states = self.linear_1(hidden_states)\n",
    "    hidden_states = self.layernorm_1(hidden_states)\n",
    "\n",
    "    # Block 2\n",
    "    hidden_states = self.linear_2(hidden_states)\n",
    "    hidden_states = self.layernorm_2(hidden_states)\n",
    "\n",
    "    logits = self.head(hidden_states)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def get_generation(model, processor, image, dtype):\n",
    "  inputs = processor(image, return_tensors=\"pt\").to(dtype)\n",
    "  out = model.generate(**inputs)\n",
    "  return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def load_image(img_url):\n",
    "    image = Image.open(requests.get(\n",
    "        img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyModel(\n",
       "  (token_embedding): Embedding(2, 2)\n",
       "  (linear_1): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (layernorm_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear_2): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (layernorm_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_dtype(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name} -- {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding.weight -- torch.float32\n",
      "linear_1.weight -- torch.float32\n",
      "linear_1.bias -- torch.float32\n",
      "layernorm_1.weight -- torch.float32\n",
      "layernorm_1.bias -- torch.float32\n",
      "linear_2.weight -- torch.float32\n",
      "linear_2.bias -- torch.float32\n",
      "layernorm_2.weight -- torch.float32\n",
      "layernorm_2.bias -- torch.float32\n",
      "head.weight -- torch.float32\n",
      "head.bias -- torch.float32\n"
     ]
    }
   ],
   "source": [
    "print_param_dtype(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Casting: float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp16 = DummyModel().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding.weight -- torch.float16\n",
      "linear_1.weight -- torch.float16\n",
      "linear_1.bias -- torch.float16\n",
      "layernorm_1.weight -- torch.float16\n",
      "layernorm_1.bias -- torch.float16\n",
      "linear_2.weight -- torch.float16\n",
      "linear_2.bias -- torch.float16\n",
      "layernorm_2.weight -- torch.float16\n",
      "layernorm_2.bias -- torch.float16\n",
      "head.weight -- torch.float16\n",
      "head.bias -- torch.float16\n"
     ]
    }
   ],
   "source": [
    "print_param_dtype(model_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.LongTensor([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6872,  0.7132],\n",
       "         [-0.6872,  0.7132]],\n",
       "\n",
       "        [[-0.6872,  0.7132],\n",
       "         [-0.6872,  0.7132]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_fp32 = model(dummy_input)\n",
    "\n",
    "logits_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logits_fp16 = model_fp16(dummy_input)\n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Casting: bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bf16 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bf16 = model_bf16.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding.weight -- torch.bfloat16\n",
      "linear_1.weight -- torch.bfloat16\n",
      "linear_1.bias -- torch.bfloat16\n",
      "layernorm_1.weight -- torch.bfloat16\n",
      "layernorm_1.bias -- torch.bfloat16\n",
      "linear_2.weight -- torch.bfloat16\n",
      "linear_2.bias -- torch.bfloat16\n",
      "layernorm_2.weight -- torch.bfloat16\n",
      "layernorm_2.bias -- torch.bfloat16\n",
      "head.weight -- torch.bfloat16\n",
      "head.bias -- torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print_param_dtype(model_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_bf16 = model_bf16(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DIFFERENCE\n",
    "\n",
    "mean_diff = torch.abs(logits_bf16 - logits_fp32).mean().item()\n",
    "\n",
    "max_diff = torch.max(logits_bf16 - logits_fp32).max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean diff: 0.000997886061668396 | Max diff: 0.0016907453536987305\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean diff: {mean_diff} | Max diff: {max_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d282ee3ef82497e89f65760cfd38a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_mem_footprint = model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint of the fp32 model in bytes:  989660400\n",
      "Footprint of the fp32 model in MBs:  989.6604\n"
     ]
    }
   ],
   "source": [
    "print(\"Footprint of the fp32 model in bytes: \",\n",
    "      fp32_mem_footprint)\n",
    "print(\"Footprint of the fp32 model in MBs: \", \n",
    "      fp32_mem_footprint/1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bf16 = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16_mem_footprint = model_bf16.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7835fe05a130>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[1;32m      3\u001b[0m img_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msfr-vision-language-research/BLIP/demo.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m display(image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m350\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(img_url)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image\u001b[39m(img_url):\n\u001b[0;32m---> 46\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/anaconda3/envs/gnn/lib/python3.9/site-packages/PIL/Image.py:3536\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3534\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3535\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3536\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7835fe05a130>"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/\\sfr-vision-language-research/BLIP/demo.jpg'\n",
    "\n",
    "image = load_image(img_url)\n",
    "display(image.resize((500, 350)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fp32 = get_generation(model, \n",
    "                              processor, \n",
    "                              image, \n",
    "                              torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fp32 Model Results:\\n\", results_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bf16 = get_generation(model_bf16, \n",
    "                              processor, \n",
    "                              image, \n",
    "                              torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bf16 Model Results:\\n\", results_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_dtype = torch.bfloat16\n",
    "\n",
    "# setting for all torch tensors\n",
    "torch.set_default_dtype(desired_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
